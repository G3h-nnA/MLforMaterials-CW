{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerated Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f8d7da; border-left: 6px solid #ccc; margin: 20px; padding: 15px;\">\n",
    "    <strong>üí° Geoffrey Hinton:</strong> It‚Äôs quite conceivable that humanity is just a passing phase in the evolution of intelligence.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe class=\"speakerdeck-iframe\" frameborder=\"0\" src=\"https://speakerdeck.com/player/e7872bd00d8348bcbf8f02720d5f36b6\" title=\"Machine Learning for Materials (Lecture 8)\" allowfullscreen=\"true\" style=\"border: 0px; background-clip: padding-box; background-color: rgba(0, 0, 0, 0.1); margin: 0px; padding: 0px; border-radius: 6px; box-shadow: rgba(0, 0, 0, 0.2) 0px 5px 40px; width: 100%; height: auto; aspect-ratio: 560 / 420;\" data-ratio=\"1.3333333333333333\"></iframe>\n",
    "\n",
    "[Lecture slides](https://speakerdeck.com/aronwalsh/mlformaterials-lecture8-ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ x üß™ Closed-loop optimisation \n",
    "\n",
    "The combination of automation and optimisation is powerful. Closed-loop workflows are of growing importance in materials research for many reasons, including:\n",
    "\n",
    "1. **Efficiency:** Efficient allocation of resources, both in terms of time and materials. By continuously updating experimental parameters based on real-time feedback, we can reduce the number of trials needed to reach optimal outcomes. \n",
    "\n",
    "2. **Adapt to changing conditions:** Adaptive decision-making, ensuring that experiments remain effective even when external factors fluctuate. This adaptability is highly valuable for complex systems where traditional trial-and-error approaches are prone to fail.\n",
    "\n",
    "3. **Exploration of large parameter spaces:** Many materials science problems involve high-dimensional parameter spaces where exhaustive exploration is impractical. Techniques such as Bayesian optimisation can efficiently sample and search these spaces to identify optimal configurations and make discoveries.\n",
    "\n",
    "4. **Data-driven insights:** Generation of valuable data from ongoing experiments. This data can be analysed to gain a deeper understanding of the underlying processes and relationships, facilitating scientific discoveries and supporting future efforts.\n",
    "\n",
    "Today we will make use of the [scikit-optimise](https://scikit-optimize.github.io) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation of libraries\n",
    "!pip install scikit-optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of modules\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.stats import norm  # Statistical functions\n",
    "from skopt import gp_minimize, dummy_minimize  # Bayesian optimisation\n",
    "from skopt.utils import create_result  # Utility functions for skopt\n",
    "from sklearn.metrics import r2_score  # R-squared metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bayesian optimisation (BO)\n",
    "\n",
    "BO is a powerful technique for optimising complex and expensive-to-evaluate functions. It combines probabilistic modeling and decision theory to search for the optimal set of parameters. In materials research, parameters like chemical composition, sample thickness, and processing conditions can be optimised.\n",
    "\n",
    "BO aims to find the global minimum (or maximum) of an objective function, $O(x)$, where $x$ represents a set of parameters or design variables. Instead of exhaustive searches, BP builds a surrogate model, typically a Gaussian Process (GP), that approximates the true objective function. This surrogate model captures both the mean $\\mu(x)$ and uncertainty $\\sigma(x)$ associated with $O(x)$. The GP is defined as:\n",
    "\n",
    "$$\n",
    "O(x) \\sim \\text{GP}(\\mu(x), k(x, x'))\n",
    "$$\n",
    "\n",
    "where $k(x, x')$ is a kernel function that quantifies the similarity between two input points $x$ and $x'$.\n",
    "\n",
    "The surrogate model balances exploration and exploitation using an acquisition function $\\alpha(x)$, which trades off between exploring uncertain regions and exploiting promising areas:\n",
    "\n",
    "$$\n",
    "x_{\\text{next}} = \\arg \\max_x \\alpha(x)\n",
    "$$\n",
    "\n",
    "Common acquisition functions include Probability of Improvement (PI), Expected Improvement (EI), and Upper Confidence Bound (UCB). Each of these functions aims to maximise the expected gain in performance over the current best solution.\n",
    "\n",
    "<details>\n",
    "<summary>Curious about the kernel function?</summary>\n",
    "\n",
    "The kernel determines the covariance structure of the GP. A commonly used kernel, and the default in `sklearn`, is the Radial Basis Function (RBF):\n",
    "\n",
    "$$\n",
    "k(x, x') = \\sigma^2 \\exp\\left(-\\frac{\\|x - x'\\|^2}{2l^2}\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\sigma^2$ is the **signal variance**, which controls the overall magnitude of function variations,\n",
    "- $l$ is the **length scale**, which determines how quickly the function values change with respect to input differences.\n",
    "\n",
    "There are also many other choices, such as the [Mat√©rn kernel](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html), which differ in how they model smoothness and continuity.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a BO model\n",
    "\n",
    "### Step 1. Target function\n",
    "\n",
    "We can start by generating a simple sine-like target function with added noise to keep things interesting. This acts as our \"virtual experiment\", i.e. we can call the function to obtain an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the target function\n",
    "def target_function(x):\n",
    "    x = np.atleast_1d(x) # Ensure x is an array\n",
    "    return np.sin(x[0]) + 0.1 * x[0] + 0.5 * np.random.randn()\n",
    "\n",
    "# Generate data for visualisation\n",
    "x_values = np.linspace(-5, 5, 200).reshape(-1, 1)\n",
    "y_values = np.vectorize(target_function)(x_values)\n",
    "\n",
    "# Plot the target function\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(x_values, y_values, 'r-', alpha=0.5, label='Target Function')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomly sample the target function and fit a simple polynomial function to get a feeling for how the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample points from the target function\n",
    "num_initial_points = 10\n",
    "initial_points = np.random.uniform(-5, 5, num_initial_points)\n",
    "initial_values = np.vectorize(target_function)(initial_points)\n",
    "\n",
    "# Plot the sample points\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(x_values, y_values, 'r-', alpha=0.5, label='Target Function')\n",
    "plt.scatter(initial_points, initial_values, color='blue', marker='o', label='Initial Samples')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Code hint </summary>\n",
    "Try `num_initial_points = 10`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a polynomial fit\n",
    "degree = 3  # Adjust the degree of the polynomial fit\n",
    "coefficients = np.polyfit(initial_points, initial_values, degree)\n",
    "poly_fit = np.poly1d(coefficients)\n",
    "\n",
    "# Calculate R^2\n",
    "y_pred = poly_fit(initial_points)\n",
    "r_squared = r2_score(initial_values, y_pred)\n",
    "\n",
    "# Plot the sample points and polynomial fit\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(x_values, y_values, 'r-', alpha=0.5, label='Target Function')\n",
    "plt.scatter(initial_points, initial_values, color='blue', marker='o', label='Initial Samples')\n",
    "plt.plot(x_values, poly_fit(x_values), 'g--', label=f'Polynomial Fit (degree {degree})\\n$R^2 = {r_squared:.4f}$')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; border-left: 6px solid #ccc; margin: 20px; padding: 15px; border-radius: 5px;\">\n",
    "    <strong>üê¢ Take a beat:</strong> Adjust the degree of the polynomial to see how good the fit is. Start with `degree = 2` and gradually increase it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Gaussian Process\n",
    "\n",
    "Now we can move to Bayesian Optimisation with a Gaussian Process model. The optimisation progress is visualised by plotting the target function, optimisation steps, and a colourbar indicating the step number.\n",
    "\n",
    "<div style=\"background-color: #d4edda; border-left: 6px solid #ccc; margin: 20px; padding: 15px; border-radius: 5px;\">\n",
    "    <strong>‚è±Ô∏è This may take a minute to run. Reverend Bayes makes computers work hard!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimise the target function using Bayesian Optimisation\n",
    "result = gp_minimize(target_function, [(-5.0, 5.0)], n_calls=50, random_state=42)\n",
    "\n",
    "# Perform random sampling for comparison\n",
    "random_result = dummy_minimize(target_function, [(-5.0, 5.0)], n_calls=50, random_state=42)\n",
    "\n",
    "# Plot the Gaussian Process model after optimisation\n",
    "x_gp = np.array(result.x_iters).reshape(-1, 1)\n",
    "y_gp = result.func_vals\n",
    "\n",
    "# Plot the target function\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(x_values, y_values, 'r-', alpha=0.5, label='Target function')\n",
    "\n",
    "# Plot the optimisation steps with a colormap\n",
    "plt.scatter(x_gp, y_gp, c=range(len(x_gp)), cmap='viridis', marker='o', label='Step number')\n",
    "\n",
    "# Add colorbar to indicate the progress\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Step number')\n",
    "\n",
    "plt.title('BO: Gaussian Process Model')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `plot_gaussian_process` from scikit-optimize to visualise the confidence intervals. `n_samples` determines the number of samples to draw from the Gaussian Process for the estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_gaussian_process as plot_gp\n",
    "\n",
    "# Plot the Gaussian Process model with confidence intervals\n",
    "plt.figure(figsize=(5, 4))\n",
    "plot_gp(result)\n",
    "\n",
    "# Add the target function for reference\n",
    "plt.plot(x_values, y_values, 'r-', alpha=0.25, label='Target function')\n",
    "\n",
    "plt.title('Confidence Intervals')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the GP mean (dashed green), confidence intervals (shaded), and sampled observations (red). The target function (light red) is also overlaid. The confidence region narrows where more observations exist and widens in unexplored areas, reflecting uncertainty in the GP model.\n",
    "\n",
    "We should always have a benchmark to compare our model to. This block extracts the best results from BO and random sampling, then compares and visualises their performance over optimisation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the cumulative minimum values\n",
    "bo_min_values = np.minimum.accumulate(result.func_vals)\n",
    "random_min_values = np.minimum.accumulate(random_result.func_vals)\n",
    "\n",
    "# Plot the cumulative minimum values vs steps for both methods\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(range(1, len(bo_min_values) + 1), bo_min_values, 'o-', label='Bayesian Optimisation')\n",
    "plt.plot(range(1, len(random_min_values) + 1), random_min_values, 'x-', label='Random Sampling')\n",
    "\n",
    "plt.title('Does BO Beat Random Sampling?')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Cumulative Minimum Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BO (blue) converges faster to a lower minimum value. Random sampling (orange) fluctuates and struggles to improve beyond a certain point. This highlights BO‚Äôs advantage in structured search over purely random exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üö® Exercise 8\n",
    "\n",
    "<div style=\"background-color: #dceefb; border-left: 6px solid #ccc; margin: 20px; padding: 15px; border-radius: 5px;\">\n",
    "    <strong>üí° Coding exercises:</strong> The exercises are designed to apply what you have learned with room for creativity. It is fine to discuss solutions with your classmates, but the actual code should not be directly copied.\n",
    "</div>\n",
    "\n",
    "### Your details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Insert your values\n",
    "Name = \"Jiaqi Wang\" # Replace with your name\n",
    "CID = 2028053 # Replace with your College ID (as a numeric value with no leading 0s)\n",
    "\n",
    "# Set a random seed using the CID value\n",
    "CID = int(CID)\n",
    "np.random.seed(CID)\n",
    "\n",
    "# Print the message\n",
    "print(\"This is the work of \" + Name + \" [CID: \" + \"0\" + str(CID) + \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "The Department of Materials has purchased a new automated thin-film deposition system. The machine has two dials that provide a 2D parameter space (x, y) for materials processing. We can define a (hypothetical) target loss function for optimising the transition temperature of our candidate thin-film superconductors as:\n",
    "\n",
    "```python\n",
    "# Target function for materials processing with x and y \"dials\"\n",
    "def supermat(inputs):\n",
    "    x, y = inputs\n",
    "    a = 2, b = 5.1 / (2 * np.pi**2)\n",
    "    c = 3 / np.pi\n",
    "    r = 4, s = 10, t = 1 / (8 * np.pi)\n",
    "\n",
    "    term1 = a * (y - b * x**2 + c * x - r)**2\n",
    "    term2 = s * (1 - t) * np.cos(x)\n",
    "    term3 = s\n",
    "\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "# Example usage:\n",
    "dials = [2.0, 3.0]\n",
    "result = supermat(dials)\n",
    "print(f\"Experiment by setting dials to ({dials[0]}, {dials[1]}): {result}\")\n",
    "```\n",
    "\n",
    "The tasks will be provided in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty block for your answers\n",
    "\n",
    "# Target function for materials processing with x and y \"dials\"\n",
    "def supermat(inputs):\n",
    "    x, y = inputs\n",
    "    a = 2\n",
    "    b = 5.1 / (2 * np.pi**2)\n",
    "    c = 3 / np.pi\n",
    "    r = 4\n",
    "    s = 10\n",
    "    t = 1 / (8 * np.pi)\n",
    "\n",
    "    term1 = a * (y - b * x**2 + c * x - r)**2\n",
    "    term2 = s * (1 - t) * np.cos(x)\n",
    "    term3 = s\n",
    "\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "# using gp_minimise for BO\n",
    "space = [(0.0, 5.0), (0.0, 5.0)]\n",
    "gp_results = gp_minimize(supermat, space, n_calls=50, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_result = dummy_minimize(supermat, [(0.0, 5.0), (0.0, 5.0)], n_calls=50, random_state=42)\n",
    "\n",
    "# print the minimum dial values and global minimum\n",
    "print(f\"The minimum (x, y) parameters are: {gp_results.x[0], gp_results.x[1]}, the global minimum is: {gp_results.fun}\")\n",
    "print(gp_results.x, gp_results.fun)\n",
    "\n",
    "# plot result in form of cumulative minimum against interations\n",
    "bo_min_values = np.minimum.accumulate(gp_results.func_vals)\n",
    "random_min_values = np.minimum.accumulate(random_result.func_vals)\n",
    "\n",
    "# Plot the cumulative minimum values vs steps for both methods\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(range(1, len(bo_min_values) + 1), bo_min_values, 'o-', label='Bayesian Optimisation')\n",
    "plt.plot(range(1, len(random_min_values) + 1), random_min_values, 'x-', label='Random Sampling')\n",
    "\n",
    "plt.title('Does BO Beat Random Sampling?')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Cumulative Minimum Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Task hint </summary>\n",
    "Remember to first define the target function and then call it using gp_minimize()\n",
    "</details>\n",
    "\n",
    "<div style=\"background-color: #d4edda; border-left: 6px solid #ccc; margin: 20px; padding: 15px; border-radius: 5px;\">\n",
    "    <strong>üìì Submission:</strong> When your notebook is complete in Google Colab, go to <em>File > Download</em> and choose <code>.ipynb</code>. The completed file should be uploaded to Blackboard under assignments for MATE70026.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåä Dive deeper\n",
    "\n",
    "* _Level 1:_ Visually explore [Gaussian Processes](https://distill.pub/2019/visual-exploration-gaussian-processes/)\n",
    "\n",
    "* _Level 2:_ Read a perspective on [Bayesian optimisation for chemical problems](https://chemrxiv.org/engage/chemrxiv/article-details/656dfe74cf8b3c3cd7c611a5) by your teaching assistant Yifan, which includes links to tool and packages under development\n",
    "\n",
    "* _Level 3:_ Interact with the self-driving laboratory demo by [Sterling Baird](https://github.com/sparks-baird/self-driving-lab-demo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
